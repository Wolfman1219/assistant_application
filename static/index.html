<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <title>Real-time VAD (16kHz Mono)</title>
  <style>
    body { font-family: sans-serif; }
    #vadLog p { margin: 0.2em 0; }
    .error { color: red; font-weight: bold; }
    .info { color: blue; }
    .event-start { color: green; }
    .event-stop { color: orange; }
    .event-error { color: red; }
    .event-connect { color: purple; }
    .event-disconnect { color: gray; }
  </style>
</head>
<body>
  <h2>üéôÔ∏è Real-Time VAD Monitor (16kHz Mono)</h2>
  <div id="status">Requesting microphone access...</div>
  <div id="vadLog"></div>

  <script>
    const logElement = document.getElementById("vadLog");
    const statusElement = document.getElementById("status");
    const socket = new WebSocket("ws://localhost:8080/ws");
    let audioContext;
    let workletNode;
    let microphoneSource;

    const TARGET_SAMPLE_RATE = 16000;

    function logMessage(type, message) {
        const p = document.createElement("p");
        p.className = `event-${type}`; // Assign class for potential styling
        p.textContent = `[${type}] ${message}`;
        logElement.appendChild(p);
        // Optional: Auto-scroll
        logElement.scrollTop = logElement.scrollHeight;
        console.log(`[${type}] ${message}`);
    }

    socket.onopen = () => {
        logMessage("connect", "WebSocket connection established.");
        startAudioProcessing(); // Start audio after WebSocket is ready
    };

    socket.onmessage = (event) => {
      try {
        const data = JSON.parse(event.data);
        // Use specific classes for VAD events if desired
        const eventType = data.event === 'VAD_START' ? 'start' : (data.event === 'VAD_END' ? 'stop' : 'info');
        logMessage(eventType, `${data.event}: ${data.message}`);
      } catch (error) {
          logMessage("error", `Failed to parse message: ${event.data}`);
          console.error("Parsing error:", error);
      }
    };

    socket.onerror = (error) => {
      logMessage("error", "WebSocket error occurred.");
      console.error("WebSocket Error:", error);
      statusElement.textContent = "WebSocket connection error. Please ensure the server is running.";
      statusElement.className = "error";
    };

    socket.onclose = (event) => {
      logMessage("disconnect", `WebSocket connection closed. Code: ${event.code}, Reason: ${event.reason || 'No reason provided'}`);
      statusElement.textContent = "WebSocket disconnected.";
      stopAudioProcessing(); // Clean up audio resources
    };

    async function startAudioProcessing() {
      try {
        statusElement.textContent = "Accessing microphone...";
        logMessage("info", "Requesting microphone access (Mono, 16kHz preferred)...");

        // 1. Request Microphone Access (Attempting Mono, 16kHz)
        // Note: Browsers might not always honor exact sampleRate constraints.
        // channelCount: 1 explicitly requests mono audio.
        const constraints = {
            audio: {
                channelCount: 1,
                sampleRate: TARGET_SAMPLE_RATE,
                echoCancellation: true, // Optional: Enable echo cancellation
                noiseSuppression: true, // Optional: Enable noise suppression
                autoGainControl: true   // Optional: Enable auto gain control
            },
            video: false
        };

        const stream = await navigator.mediaDevices.getUserMedia(constraints);
        logMessage("info", "Microphone access granted.");
        statusElement.textContent = "Microphone active.";

        // 2. Create Audio Context (Attempting 16kHz)
        // We try to create the context with the desired sample rate.
        try {
            audioContext = new AudioContext({ sampleRate: TARGET_SAMPLE_RATE });
            logMessage("info", `AudioContext created. Requested sample rate: ${TARGET_SAMPLE_RATE}Hz. Actual rate: ${audioContext.sampleRate}Hz.`);
            if (audioContext.sampleRate !== TARGET_SAMPLE_RATE) {
                 logMessage("info", `Note: Browser did not provide the requested sample rate. Using ${audioContext.sampleRate}Hz. The Worklet will process at this rate.`);
                 // IMPORTANT: If the server *strictly* requires 16kHz, you'd need
                 // to implement resampling within the AudioWorkletProcessor.
                 // For now, we proceed with the browser's provided rate.
            }
        } catch (e) {
            logMessage("warning", `Could not create AudioContext with ${TARGET_SAMPLE_RATE}Hz. Falling back to default rate.`);
            console.warn("Sample rate constraint failed:", e);
            audioContext = new AudioContext(); // Create with default settings
             logMessage("info", `AudioContext created with default sample rate: ${audioContext.sampleRate}Hz.`);
             // Again, resampling would be needed in the worklet if 16kHz is mandatory.
        }


        // 3. Add the AudioWorklet Module
        await audioContext.audioWorklet.addModule('audio-processor.js');
        logMessage("info", "AudioWorklet processor loaded.");

        // 4. Create MediaStreamSource and AudioWorkletNode
        microphoneSource = audioContext.createMediaStreamSource(stream);
        workletNode = new AudioWorkletNode(audioContext, 'audio-processor');
         logMessage("info", "Audio nodes created.");

        // 5. Handle messages (audio buffers) from the Worklet
        workletNode.port.onmessage = (event) => {
          // event.data is the Int16Array buffer sent from the processor
          if (socket.readyState === WebSocket.OPEN) {
            socket.send(event.data); // Send the raw ArrayBuffer
          }
        };

        workletNode.port.onerror = (error) => {
            logMessage("error", "Error occurred in AudioWorklet processor.");
            console.error("AudioWorklet Error:", error);
        };

        // 6. Connect the audio graph: Microphone -> Worklet
        // We don't *need* to connect to audioContext.destination unless you want to hear the input.
        microphoneSource.connect(workletNode);
        // Optional: Connect to output to hear the microphone input
        // workletNode.connect(audioContext.destination);

        logMessage("info", "Audio processing pipeline connected and running.");
        statusElement.textContent = `Microphone active (Processing at ${audioContext.sampleRate}Hz)`;
        statusElement.className = "info";


      } catch (err) {
        logMessage("error", `Error starting audio processing: ${err.message}`);
        console.error("Audio Setup Error:", err);
        statusElement.textContent = `Error: ${err.name} - ${err.message}`;
        statusElement.className = "error";
        // Offer guidance
        if (err.name === 'NotAllowedError' || err.name === 'PermissionDeniedError') {
            statusElement.textContent += " Please grant microphone permission.";
        } else if (err.name === 'NotFoundError' || err.name === 'DevicesNotFoundError') {
            statusElement.textContent += " No microphone found.";
        } else if (err.name === 'NotReadableError' || err.name === 'TrackStartError') {
             statusElement.textContent += " Microphone might be already in use by another application.";
        } else if (err.name === 'OverconstrainedError' || err.name === 'ConstraintNotSatisfiedError') {
             statusElement.textContent += ` Could not satisfy audio constraints (e.g., sample rate ${TARGET_SAMPLE_RATE}Hz or mono). Try relaxing constraints.`;
        }
      }
    }

    function stopAudioProcessing() {
        if (microphoneSource) {
            microphoneSource.disconnect();
            microphoneSource.mediaStream.getTracks().forEach(track => track.stop()); // Stop mic tracks
            microphoneSource = null;
             logMessage("info", "Microphone source disconnected and tracks stopped.");
        }
        if (workletNode) {
            workletNode.disconnect();
            workletNode.port.close();
            workletNode = null;
            logMessage("info", "AudioWorklet node disconnected.");
        }
        if (audioContext && audioContext.state !== 'closed') {
            audioContext.close().then(() => {
                logMessage("info", "AudioContext closed.");
                audioContext = null;
             });
        }
         statusElement.textContent = "Audio processing stopped.";
    }

    // Clean up on page unload
    window.addEventListener('beforeunload', () => {
        stopAudioProcessing();
        if (socket && socket.readyState === WebSocket.OPEN) {
            socket.close();
        }
    });

  </script>
</body>
</html>